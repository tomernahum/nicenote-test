


TODO Next:
- Crypto
- batching

TODO:
- clean up the console logs
- ask AI for tips on code quality, api design

- fix bug: since socketio works globally if we close our connection in one docProvider it will close connection in all doc providers for that same doc. easy fix by just keeping track of callbacks in 2-server-interface

TODO Later:
- fix up snapshotting to be more intelligent, don't make duplicate snapshots, don't make them too frequently, maybe notify clients of new snapshots to help with them deciding whether to snapshot (may fix the above on its own)


- publish this in it's own repo
- publish this on npm
- write up what is and isn't in the threat model / accomplished by this system. And what can be toggled

- abstract the buckets for dynamic amount of buckets (should be easy right?)


--
Things that are not hidden from server and not prevented from being changed by server
- rowId
- (considering) update encryption schema version id
- side channels / implicit
    - which IP made an update
        - idk if server can get other metadata about the caller too, if it were just an http api and client not intentionally sharing more
        - our client is socketio so also sharing socket id, stuff I might not know about
        - browser also does automatically send other metadata like user agent header and referer header 
            - I should try to strip out what I can I guess
        - I won't intentionally put fingerprinting code on my frontend at least though.. though other lib consumers might. and I will put optional analytics actually
        - basically server knows who you are unless you are actively using tor. probably knows which user you are out of people on doc too. which is fine probably
    - message size increments (small medium large, xl, etc)
        - increments and not exact lengths thanks to padding. less increments the less is leaked but the more unnecessary data we are sending around
    - timing / speed of updates
        - going to try to mitigate this by doing message batching. basically do padding for time. tradeoff is less responsive updates. could be cool to embed playback data in the update so typing is shown same as it was entered, just delayed, instead of all at once every 0.25 second snapshot. but either way
        - could leak what you are typing if you type certain things faster (eg spam delete vs typing sentence) 

        - together with message size increments will definitely leak whether you are pasting large block vs typing characters
            - but at least content will not be leaked. 
              also not like I expect the server to be scanning for that (I do expect a given small time admin to read through user data, and a big time app to be subpoenaed for user data, or employee of a big company to snoop)

- server can also selectively omit updates without user knowing
    - hope to find mitigation eventually     



---
Mitigations:
- batching for timing side channel

Remaining problems
- need mitigation for server disrupting integrity by omitting subset of updates
    - eg tie updates to creator, tie that to user identity, then soft block that one user
- (may not solve): server knows who made an update based on ip address, may know more about identity
    - maybe p2p / websockets will help with this?



-----Oldish---------


- intelligent snapshot rejection on server?

- notify clients of new snapshots?
    - waste of bandwidth if snapshots are really correct
- periodic reloads of doc just in case it diverges
    - related to above
    - balance of bandwidth usage vs safety
- only send snapshot if document is too long including snapshots?
    - need to notify client of existence of new snapshots for this (not necessarily send the whole content down...)


- proper cryptography key stuff.
    - support multiple keys & while-running key rotation
    - write keys
maybe: writer id or writer key id recorded in operation. later may add custom resolving crdt-like which takes this into account
    for now plan was to have one shared write-key. we could give each writer their own key per document = almost same as identity 

- error handling down to user (eg max server message size)
- clean up console logs

- deploy for easier cross-device testing


- batching of updates to mask frequency of them from server






- server: think about spam prevention story maybe? & integration with a proper authed / customized extendible server (besides cryptography auth)
    - maybe turn backend into library or something idk

- is there a way to replace server with webrtc + storage endpoint for more decentralization/openness, but more importantly to save on egress charges?
    is ingress of storage of new updates free? , and only loading the doc (maybe even only while noone else is online) has to be egress

- make sure it is as bug free as possible

- restructure folders and stuff

- publish this

Maybe:
- delete/wipe doc functionality? (you can just squash empty doc in prod and delete from sql viewer in dev though)
- maybe deprecate connect to doc flow in 2-server-interface, since we want to fetch it before connecting to socket part
- or not. it's possible I got confused thinking that http is faster than spinning up websocket.

later: make buckets dynamic
& add other crdt libraries / custom (loro tree & reducer-based)

--


Make sure everything is good

--
- could abstract bucket from "doc"|"awareness" to be any string, to more easily connect to other crdts or even ordered reducers